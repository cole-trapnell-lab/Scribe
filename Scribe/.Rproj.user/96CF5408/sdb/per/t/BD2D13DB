{
    "collab_server" : "",
    "contents" : "#function to generate new cds: \nmake_cds <- function (exprs_matrix, pd, fd, expressionFamily) {\n  cds <- newCellDataSet(exprs_matrix, \n                        phenoData = new(\"AnnotatedDataFrame\", data = pd), \n                        featureData = new(\"AnnotatedDataFrame\", data = fd), \n                        expressionFamily = expressionFamily, \n                        lowerDetectionLimit = 0.1)\n  \n  if(identical(expressionFamily, tobit())) {\n    cds <- estimateSizeFactors(cds)\n    cds <- estimateDispersions(cds)\n  }\n  return(cds)\n}\n\n# function to reproduce the smooth used by Arman\n# exprs: row: cell; column: gene\nmove_avg_f <- function(exprs, window_size = 40) {\n  win_range <- nrow(exprs) - window_size\n  exprs_smooth <- exprs[-c(1:window_size), ]\n  \n  res <- apply(exprs, 2, function(x) {\n    tmp <- rep(0, win_range)\n    for(i in 0:(win_range)){\n      tmp[i + 1] <- mean(x[i + c(1:window_size)])\n    }\n    return(tmp)\n  })\n  \n  return(res)\n}\n\n# function to calculate direct information\ncalculate_and_write_pairwise_di <- function(genes_data, delays = c(1,2,5,10,15,20,25), supergraph = NULL, cores = 1, verbose = F){\n  if(verbose) print(\"Calculating the Directed Mutual Information for each pair of genes...\")\n  \n  cnt <- 0\n  N_operations <- nrow(genes_data) * (nrow(genes_data) - 1)\n  \n  pairwise_dmi_results <- c()\n  pairwise_corr_results <- c()\n  \n  tmp <- expand.grid(colnames(genes_data), colnames(genes_data), stringsAsFactors = F)\n  all_pairwise_gene <- tmp[as.character(tmp[, 1]) != as.character(tmp[, 2]), ] #don't calculate RDI on self-loop\n  \n  if(!is.null(supergraph)) { #consider the supergraph\n    all_pairwise_gene_paste <- paste(all_pairwise_gene[, 1], all_pairwise_gene[, 2])\n    supergraph_paste <- paste(supergraph[, 1], supergraph[, 2])\n    all_pairwise_gene <- all_pairwise_gene[all_pairwise_gene_paste %in% supergraph_paste, ]\n  }\n  \n  all_pairwise_gene_list <- split(all_pairwise_gene, row.names(all_pairwise_gene))\n  \n  #we may need to convert genes_data into sparseMatrix before passing to mclapply\n  res <- mclapply(all_pairwise_gene_list, function(x, genes_data, delays, N_operations) {\n    di <- computeTE(genes_data[, x[[1]]] + rnorm(nrow(genes_data), sd = 1e-12), genes_data[, x[[2]]] + rnorm(nrow(genes_data), sd = 1e-12), embedding = 3, k = 3, safetyCheck = T)\n    #computeTE(genes_data[, x[[1]]], genes_data[, x[[2]]], embedding = 3, k = 3) #calculate_rdi_corr\n    res <- data.frame(id_1 = x[[1]], id_2 = x[[2]],  delay = di$TE)\n    colnames(res)[3] <- paste('delays ', delays, sep = '')\n    res\n  }, genes_data = genes_data, delays = delays, N_operations = N_operations, mc.cores = cores)\n  \n  res <- do.call(rbind.data.frame, res)\n  row.names(res) <- paste(res$id_1, res$id_2, sep = '_')\n  return(res)\n}\n\n#function to run the ccm and save the result\ncal_cross_map <- function(ordered_exprs_mat, lib_colum, target_column, RNGseed = 2016, window_size = 1) { \n  if(window_size != 1)\n    ordered_exprs_mat <- move_avg_f(ordered_exprs_mat, window_size)\n  lib_xmap_target <- ccm(ordered_exprs_mat, E = 2, random_libs = TRUE, lib_column = lib_colum, #ENSG00000122180.4\n                         target_column = target_column, lib_sizes = 25, num_samples = 25, RNGseed = RNGseed) #seq(10, 75, by = 5), num_samples = 300,\n  target_xmap_lib <- ccm(ordered_exprs_mat, E = 2, random_libs = TRUE, lib_column = target_column, #ENSG00000122180.4\n                         target_column = lib_colum, lib_sizes = 25, num_samples = 25, RNGseed = RNGseed)\n  \n  lib_xmap_target_means <- ccm_means(lib_xmap_target)\n  target_xmap_lib_means <- ccm_means(target_xmap_lib)\n  \n  return(list(lib_xmap_target = lib_xmap_target, target_xmap_lib = target_xmap_lib, \n              lib_xmap_target_means = lib_xmap_target_means, target_xmap_lib_means = target_xmap_lib_means,\n              mean_lib_xmap_target_means = mean(lib_xmap_target_means$rho[is.finite(lib_xmap_target_means$rho)], na.rm = T), \n              mean_target_xmap_lib_means = mean(target_xmap_lib_means$rho[is.finite(target_xmap_lib_means$rho)], na.rm = T)))\n}\n\n#function to plot the result from ccm\nplot_cross_map <- function(lib_xmap_target_means, target_xmap_lib_means, lib_name, target_name){\n  legend_names <- c(paste(lib_name, 'xmap', target_name), paste(target_name, 'xmap', lib_name))\n  \n  xmap_all <- rbind(lib_xmap_target_means, target_xmap_lib_means)\n  xmap_all$type <- c(rep('a_xmap_t_means', nrow(lib_xmap_target_means)), rep('t_xmap_a_means', nrow(target_xmap_lib_means)))\n  y_max <- max(xmap_all$rho, na.rm = T) + 0.1\n  \n  lib_rng <- range(xmap_all$lib_size)\n  p1 <- ggplot(aes(lib_size, pmax(0, rho)), data = xmap_all) + geom_line(aes(color = type)) + xlim(lib_rng) + \n    xlab(\"Library Size\") + ylab(\"Cross Map Skill (rho)\") + scale_color_discrete(labels=legend_names) + \n    scale_x_discrete(breaks = unique(xmap_all$lib_size)) + monocle_theme_opts()\n  \n  return(p1)\n}\n\n#parallel the CCM algorithm: \nparallelCCM <- function(ordered_exprs_mat, cores = detectCores() / 2, window_size = 20) {\n  ordered_exprs_mat <- ordered_exprs_mat\n  combn_mat <- combn(1:ncol(ordered_exprs_mat), 2)\n  \n  combn_mat_split <- split(t(combn_mat), 1:ncol(combn_mat))\n  CCM_res <- mclapply(combn_mat_split, function(x, ordered_exprs_mat){ \n    col_names <- colnames(ordered_exprs_mat)[x]\n    cross_map_res <- cal_cross_map(ordered_exprs_mat[, col_names], col_names[1], col_names[2])\n    cross_map_res[c('lib_xmap_target_means', 'target_xmap_lib_means')]\n  }, ordered_exprs_mat = ordered_exprs_mat, mc.cores = cores)\n  \n  return(list(CCM_res = CCM_res, combn_mat_split = combn_mat_split, gene_names = colnames(ordered_exprs_mat)))\n}\n\n#function to prepare the result for CCM: \nprepare_ccm_res <- function(parallel_res, gene_names = NULL){\n  if(is.null(gene_names))\n    gene_names <- parallel_res$gene_names\n  parallel_res_list <- lapply(1:length(parallel_res$CCM_res), function(x, gene_names) {\n    lib_xmap_target_means <- parallel_res$CCM_res[[x]]$lib_xmap_target_means #mean of mean under different library\n    target_xmap_lib_means <- parallel_res$CCM_res[[x]]$target_xmap_lib_means #mean of mean under different library\n    lib_name <- parallel_res$combn_mat_split[[x]][1] \n    target_name <- parallel_res$combn_mat_split[[x]][2] \n    data.frame(lib_name = c(gene_names[lib_name], gene_names[target_name]),\n               target_name = c(gene_names[target_name], gene_names[lib_name]),\n               mean_rho = c(lib_xmap_target_means, target_xmap_lib_means))\n  }, as.character(gene_names))\n  \n  parallel_res_df <- do.call(rbind.data.frame, parallel_res_list)\n  parallel_res_mat <- dcast(parallel_res_df, lib_name ~ target_name, fun.aggregate=mean)\n  \n  row.names(parallel_res_mat) <- parallel_res_mat$lib_name\n  parallel_res_mat <- parallel_res_mat[, -1]\n  \n  parallel_res_mat[!is.finite(as.matrix(parallel_res_mat))] <- 0\n  diag(parallel_res_mat) <- 0\n  \n  return(parallel_res_mat)\n}\n\n#output format: Gene_1_ID Gene_1_NAME Gene_2_ID Gene_2_NAME delay_max RDI\nparallel_cal_grangertest <- function(exprs_data, cores =  detectCores() - 2, delays = 1, smoothing = T, filename = 'granger_res.txt') {\n  gene_name <- colnames(exprs_data)\n  combn_df <- combn(gene_name, m = 2)\n  split_combn_df <- split(t(combn_df), rep(1:ncol(combn_df), nrow(combn_df)))\n  \n  grangertest_res <- mclapply(split_combn_df, function(x, fname, dl, smooth = smoothing) {\n    subset_df <- exprs_data[, c(x)]\n    if(smooth){\n      subset_df <- move_avg_f(subset_df, window_size = 10)\n      subset_df <- as.data.frame(subset_df)\n    }\n    colnames(subset_df) <- c('x0', 'x1')\n    \n    res_df <- tryCatch({\n      res <- cal_grangertest(subset_df, delays = dl)\n      res\n    }, error = function(e){\n      print(e)\n      res <- data.frame(x0_by_x1 = -100,  x1_by_x0 = -100) #change this to -100 for cases where we throw numerical errors \n      \n      res\n    })\n    \n    df <- data.frame(Gene_1_ID = c(x[1], x[2]), Gene_1_NAME = c(x[1], x[2]), Gene_2_ID = c(x[2], x[1]), Gene_2_NAME = c(x[2], x[1]), delay_max = NA, \n                     granger = c(mean(res_df$x1_by_x0), mean(res_df$x0_by_x1)) )\n    write.table(file = fname, df, append = T, row.names = F, col.names = F, quote = F)\n    return(df)\n  }, mc.cores = cores, fname = filename, dl = delays, smooth = smoothing)\n  \n  return(grangertest_res)\n}\n\n#functions to perform granger tests using lmtest package: \n# add new options to select a range of delays: \ncal_grangertest <- function(ordered_exprs_mat, delays = 1) {\n  df <- data.frame(ordered_exprs_mat)\n  x0_by_x1 <- rep(0, length(delays))\n  x1_by_x0 <- rep(0, length(delays))\n  \n  for(i in 1:length(delays)) {\n    \n    x0_by_x1[i] <- grangertest(x0 ~ x1, order = delays[i], data = df)$`Pr(>F)`[2]\n    x1_by_x0[i] <- grangertest(x1 ~ x0, order = delays[i], data = df)$`Pr(>F)`[2]    \n  }\n  \n  return(data.frame(x0_by_x1 = max(x0_by_x1), \n                    x1_by_x0 = max(x1_by_x0)))\n}\n\n#functions to perform granger tests using VAR package: \ncal_grangertest_var <- function(ordered_exprs_mat, order = 1) {\n  var <- VAR(ordered_exprs_mat, p = order, type = \"const\")\n  x1_by_x0 <- causality(var, cause = \"x0\")$Granger\n  x0_by_x1 <- causality(var, cause = \"x1\")$Granger\n  \n  return(data.frame(x0_by_x1 = x0_by_x1$p.value, \n                    x1_by_x0 = x1_by_x0$p.value))\n}\n\n#function to implement the so called MAGIC method: \ndiffusion_maps <- function (data, bw_ini = 0, pseudo_cnt = 1, neighbours = 0.2,\n                            log2_data = F, max_components = 3) {\n  if (log2_data)\n    data <- t(log2(data + pseudo_cnt))\n  else data <- t(data)\n  data_dm_bw_res <- diffusion_maps_bw(data, pseudo_cnt = pseudo_cnt,\n                                      neighbours = neighbours, bw_ini = 0, iter = 100, step = 0.02,\n                                      log2_data = log2_data)\n  bw = 10^(0.2 * (which(data_dm_bw_res$av_d_sigma == max(data_dm_bw_res$av_d_sigma)) +\n                    1))\n  nn <- ceiling(nrow(data) * neighbours)\n  d2 <- as.matrix(dist(data)^2)\n  sigma <- bw^2\n  W <- exp(-d2/(2 * sigma))\n  R <- apply(d2, 2, function(x) sort(x)[nn])\n  R <- matrix(rep(R, ncol(d2)), ncol = ncol(d2))\n  W <- (d2 < R) * W\n  W <- W + t(W)\n  D <- colSums(W, na.rm = T)\n  q <- D %*% t(D)\n  diag(W) <- 0\n  H <- W/q\n  colS <- colSums(H)\n  Hp <- t(t(H)/colS)\n  E <- eigen(Hp)\n  eigOrd <- order(Re(E$values), decreasing = TRUE)\n  E$values <- E$values[eigOrd][-1]\n  E$vectors <- E$vectors[, eigOrd][, -1]\n  rownames(E$vectors) <- rownames(data)\n  colnames(E$vectors) <- 1:ncol(E$vectors)\n  diffMap <- t(E$vectors)\n  return(t(diffMap[1:max_components, ]))\n}\n\nget_ka_dist <- function(X, K = 5) {\n  N <- ncol(X)\n  norm_sq <- repmat(t(colSums(X^2)), N, 1)\n  dist_sq <- norm_sq + t(norm_sq) - 2 * t(X) %*% X\n  sort_idx <- t(apply(dist_sq, 2, function(x) sort(x, index.return = T)$ix ))\n  knn_idx <- sort_idx[, c(1, K + 1)]\n  \n  distance <- apply(knn_idx, 1, function(ind_vec) sqrt(sum((X[, ind_vec[1]] - X[, ind_vec[2]])^2)) )\n  \n  return(distance)\n}\n\n#implement based on DPT\n# D: row: cell; column genes\nlibrary(RANN)\nlibrary(expm) \nMAGIC_R <- function(D, kernel='gaussian', n_pca_components=2, random_pca=True, \n                    t=6, knn=30, knn_autotune=10, epsilon=1, rescale=99, k_knn=100, perplexity=30,\n                    var_explained = 0.75) {\n  #library size normalization\n  Libsize <- rowSums(D)\n  D_norm <- D / Libsize * median(Libsize)\n  \n  #PCA\n  res <- prcomp(D_norm, center = T, scale = F)\n  \n  #select only the top \n  std_dev <- res$sdev \n  pr_var <- std_dev^2\n  prop_varex <- pr_var/sum(pr_var)\n  \n  D_pca <- res$x[, 1:max(n_pca_components, min(which(cumsum(prop_varex) >= var_explained)))]\n  \n  # #ğ·ğ‘–ğ‘ ğ‘¡ = ğ‘ğ‘œğ‘šğ‘ğ‘¢ğ‘¡ğ‘’_ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’_ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥(ğ·)\n  # Dist <- as.matrix(dist(D_pca))\n  # # ğœ ğ‘– ï¿¼ = ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘–, ğ‘›ğ‘’ğ‘–ğ‘”hğ‘ğ‘œğ‘Ÿ ğ‘–, ğ‘˜ğ‘ )\n  # sigma <- get_ka_dist(X = t(D_pca), K = ka)\n  # #ğ´ = ğ‘ğ‘œğ‘šğ‘ğ‘¢ğ‘¡ğ‘’_ğ‘ğ‘“ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘¦_ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥(ğ·ğ‘–ğ‘ ğ‘¡)\n  # A <- exp(- (Dist / sigma)^2 )\n  # A <- A + t(A)\n  # #ğ‘€ = ğ‘ğ‘œğ‘šğ‘ğ‘¢ğ‘¡ğ‘’_ğ‘šğ‘ğ‘Ÿğ‘˜ğ‘œğ‘£_ğ‘ğ‘“ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘¦_ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥(ğ´)\n  # M <- A / rowSums(A)\n  \n  L <- compute_markov(D_pca, knn=knn, epsilon=epsilon, \n                      distance_metric='euclidean', knn_autotune=knn_autotune)\n  \n  #D_imputed = M^t * D\n  D_imputed <- as.matrix(L) %^% t %*% D\n  #D_rescaled = Rescale(D_imputed)\n  D_rescaled <- t(t(D_imputed) * (apply(D, 2, function(x) quantile(x, 0.99)) ) / rowMax(t(D_imputed)))\n  #Dimputed = D_rescaled\n  return(D_rescaled)\n}\n\n# update the section on calculating the Markov matrix\ncompute_markov <- function(data, knn = 10, epsilon = 1, distance_metric = 'euclidean', knn_autotune = 0) {\n  N <- nrow(data)\n  #Nearest neighbors\n  nbrs <- RANN::nn2(data, k = knn)\n  distances <- nbrs$nn.dists\n  indices =  nbrs$nn.idx\n  if(knn_autotune > 0) {\n    print('Autotuning distance')\n    \n    for(j in rev(1:N)) {\n      temp <- sort(distances[j, ])\n      lMaxTempIdxs = min(knn_autotune + 1, length(temp))\n      if(lMaxTempIdxs == 1 | temp[lMaxTempIdxs] == 0)\n        distances[j, ] <- 0\n      else\n        distances[j, ] <- distances[j, ] / temp[lMaxTempIdxs]\n    }\n  }\n  \n  rows <- rep(0, N * knn)\n  cols <- rep(0, N * knn)\n  dists <- rep(0, N * knn)\n  location <- 1\n  \n  for(i in 1:N) {\n    inds <- location:(location + knn - 1)\n    rows[inds] <- indices[i, ]\n    cols[inds] <- i\n    dists[inds] <- distances[i, ]\n    location <- location + knn\n  }\n  \n  if(epsilon > 0)\n    W <- sparseMatrix(rows, cols, x = dists, dims = c(N, N))\n  else\n    W <- sparseMatrix(rows, cols, x = rep(1, nrow(dist) * ncol(dist)), dims = c(N, N))\n  \n  #Symmetrize W\n  W <- W + t(W)\n  if(epsilon > 0){\n    # Convert to affinity (with selfloops)\n    tmp <- which(as.matrix(W) > 0, arr.ind = T)\n    rows <- tmp[, 1]\n    cols <- tmp[, 2]\n    dists <- W[tmp]\n    rows <- c(rows, 1:N)\n    cols <- c(cols, 1:N)\n    dists <- c(dists/(epsilon^2), rep(0, N))\n    W <- sparseMatrix(rows, cols, x = exp(-dists), dims = c(N, N))\n  }\n  \n  # Create D\n  D <- rowSums(W)\n  D[D != 0] <- 1 / (D[D != 0])\n  \n  #markov normalization\n  T <- sparseMatrix(1:N, 1:N, x = D, dims = c(N, N)) %*% W\n  return(T)\n}\n\n#   function(D, kernel='gaussian', n_pca_components=20, random_pca=True, \n#                     t=6, k#run roc curve\ngenerate_roc_df <- function(p_value, classification, type = 'fpr') {\n  library(ROCR)\n  p_value[is.na(p_value)] <- 1\n  pred_p_value <- prediction(p_value, classification)\n  perf_tpr_fpr <- performance(pred_p_value, \"tpr\", \"fpr\")\n  \n  fpr = perf_tpr_fpr@x.values\n  \n  tpr = perf_tpr_fpr@y.values\n  \n  perf_auc <- performance(pred_p_value, \"auc\")\n  auc <- perf_auc@y.values\n  \n  data.frame(tpr = tpr, fpr = fpr, auc = auc)\n}\n\n# simulate the dropout and convert to poisson or negative binomial distribution \n# simulate <- function(E,# simulate the dropout and convert to poisson or negative binomial distribution# # #  \n# simulate <- function(E,){\n#   \n# ar_explained = 0.75) {\n#   #library size normalization\n#   Libsize <- rowSums(D)\n#   D_norm <- D / Libsize * median(Libsize)\n# \n#   #PCA\n#   res <- prcomp(D_norm, center = T, scale = F)\n#   \n#   #select only the top \n#   std_dev <- res$sdev \n#   pr_var <- std_dev^2\n#   prop_varex <- pr_var/sum(pr_var)\n#   \n#   D_pca <- res$x[, 1:max(n_pca_components, min(which(cumsum(prop_varex) >= var_explained)))]\n#   \n#   # #ğ·ğ‘–ğ‘ ğ‘¡ = ğ‘ğ‘œğ‘šğ‘ğ‘¢ğ‘¡ğ‘’_ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’_ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥(ğ·)\n#   # Dist <- as.matrix(dist(D_pca))\n#   # # ğœ ğ‘– ï¿¼ = ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘–, ğ‘›ğ‘’ğ‘–ğ‘”hğ‘ğ‘œğ‘Ÿ ğ‘–, ğ‘˜ğ‘ )\n#   # sigma <- get_ka_dist(X = t(D_pca), K = ka)\n#   # #ğ´ = ğ‘ğ‘œğ‘šğ‘ğ‘¢ğ‘¡ğ‘’_ğ‘ğ‘“ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘¦_ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥(ğ·ğ‘–ğ‘ ğ‘¡)\n#   # A <- exp(- (Dist / sigma)^2 )\n#   # A <- A + t(A)\n#   # #ğ‘€ = ğ‘ğ‘œğ‘šğ‘ğ‘¢ğ‘¡ğ‘’_ğ‘šğ‘ğ‘Ÿğ‘˜ğ‘œğ‘£_ğ‘ğ‘“ğ‘“ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘¦_ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥(ğ´)\n#   # M <- A / rowSums(A)\n#   \n#   L <- compute_markov(D_pca, knn=knn, epsilon=epsilon, \n#                  distance_metric='euclidean', knn_autotune=knn_autotune)\n#   \n#   #D_imputed = M^t * D\n#   D_imputed <- L^t %*% D\n#   #D_rescaled = Rescale(D_imputed)\n#   D_rescaled <- D_imputed * (apply(D, 1, function(x) as.matrix(t)quanile(x, 0.99)) ) / rowMax(D_imputed)\n#   \n#   #Dimputed = D_rescaled\n#   return(D_rescaled)\n# }\n\n###############################################################################################################################################################################################\n#run pscl's zeroinfl function to calculate the expected zero values \n###############################################################################################################################################################################################\n# #use the moving average with the hurdle/zeroinfl model for data fitting:\n# pscl_smooth_genes <- function(exprs, window_size = 40) {\n#   win_range <- nrow(exprs) - window_size\n#   exprs_smooth <- exprs[-c(1:windo #use the moving average with the hurdle/zeroinfl model for data fitting# pscl_smooth_genes <- function(exprs, window_size = 40) {\n#   win_range <- nrow(exprs) - window_size\n#   exprs_smooth <- exprs[-c(1:windo #use the moving average with the hurdle/zeroinfl model for data fitting:\n#  {\n#     tmp <- rep(NA, # pscl_smooth_genes <- function(exprs, window_size = 40) {\n# #   win_range <- nrow(exprs) - window_size\n# = round(x[i + c(1:window_size)]))\n#       # print(x)\n#       tryCatch({\n#         tmp[i + 1] <- zeroinfl(expression ~ 1, data = df)$fitted.values\n#         tmp\n#       },\n#       #warning = function(w) { FM_fit },\n#       error = function(e) {\n#         # tmp[i + 1] <- glm.nb(expression ~ 1, data = df)$fitted.values\n#         # tmp\n#       })\n#       if(is.na(tmp[i + 1]))\n#         tmp[i + 1] <- glm.nb(expression ~ 1, data = df)$fitted.values\n#     }\n#     return(tmp)\n#   })\n#   \n#   return(res)\n# th <- exprs[-c(1:window_size), ]\n\n# res <- apply(exprs, 2, function(x) {\n#   tmp <- rep(NA, win_range)\n#   for(i in 0:(win_range)){\n#     df <- data.frame(expression = round(x[i + c(1:window_size)]))\n#     # print(x)\n#     tryCatch({\n# # ## res <- apply(exprs, 2, function(x) {\n#   tmp <- rep(NA, win_range)\n#   for(i in 0:(win_range)){\n#     df <- data.frame(expression = round(x[i + c(1:window_size)]))\n#     # print(x)\n#print(x)\n#      tryCatch({\n#print(x)\n#       tryCatch({\n#         tmp[i + 1] <- exp(zeroinfl(expression ~ 1, data = df)$coefficients$count)\n#         tmp\n#       },\n#       #warning = function(w) { FM_fit },\n#       error = function(e) {\n#         # tmp[i + 1] <- glm.nb(expression ~ 1, data = df)$fitted.values\n#         # tmp\n#       })\n#       if(is.na(tmp[i + 1]))\n#         tmp[i + 1] <- glm.nb(expression ~ 1, data = df)$fitted.values\n#     }\n#     return(tmp)\n#   })\n#   \n#   return(res)\n# # graph \n#   g <- graph_from_adjacency_matrix(as.matrix(RDI_matrix), mode = \"directed\", weighted = T)\n\n#   #normalize the weight by the maximal incoming weight: \n#   neighbor_nodes <- neighborhood(g, mode = mod# graph \n#   g <- graph_from_adjacency_matrix(as.matrix(RDI_matrix), mode = \"directed\", weighted = T)\n#   \n#   #normalize the weight by the maximal incoming weight: \n#   neighbor_nodes <- neighborhood(g, mode = mode)\n#   names(neighbor_nodes) <- V(g)$name\n#   \n#   for(i in names(neighbor_nodes)) {\n#     if(mode == 'in') {\n#       edge_name <- paste(neighbor_nodes[[i]]$name, i, sep = '|') #get the\n#     }\n#     else if(mode == 'out') {\n#       edge_name <- paste(i, neighbor_nodes[[i]]$name, sep = '|') #get the\n#     }\n#     \n#     weight_vec <- E(g1)[edge_name]$weight\n#     \n#     weight_vec_norm <- weight_vec / max(weight_vec)\n#     E(g1)[edge_name]$weight <- weight_vec_norm #normalize the expression by the maximal value \n#     \n#     RDI_matrix[neighbor_nodes[[i]]$name, i] <- weight_vec_norm\n#   }\n#   \n#   return(list(normalized_rdi_matrix = RDI_matrix, g = g))\n# dd theta \n#   res <- apply(sim_data, 2, function(x) {\n#     tmp <- x / sum(x)\n#     tmp <- round(tmp * avg_transcripts / mean(tmp))\n#   })\n\n#   unique_counts <- unique(unlist(as.data.frame(res)))\n#   for(i in names(unique_counts)){\n#     #dropout_rate = (1 - c)^n\n#     tmp_vec <- rep(as.numeric(i), unique_counts[i])\n#     tmp_vec[sample(unique_counts[i], (1 - capture_rate)^as.numeric(i))] <- 0 \n#     res[unique_counts == as.numeric(i)] <- tmp_vec\n#   }\n\n#   #sample_ind <- sample(1:length(tmp), dropout_rate * length(tmp))\n#   #tmp[sample_ind] <- 0 \n\n#   res <- apply(res * capture_rate, 1, function(x, mode = method, n_cell = ncol(res)) {\n#     if(mode == 'rank') {\n#       order_ind <- order(x)\n#       x[order_ind] <- sort(rpois(n = n_cell, lambda = mean(x)))\n#     }\n#     else if(mode == 'naive') {\n#       for(y in 1:length(x))\n#         x[y] <- rpoisson(n = 1, lambda = x[y])\n#     }\n\n#     x\n#   })\n\n#   return(res)\n# }\n\n# functions to process the RDI network when we get it: \n\nquantile_selection_network <- function(rdi_network, prob = 0.95, experiment) \n{\n  if(class(rdi_network) != 'data.frame')\n    stop('the rdi network should be a data frame with row / column names included')\n  \n  rdi_network_q <- quantile(unlist(rdi_network), probs = prob)\n  rdi_network_q_process <- rdi_network; rdi_network_q_process[rdi_network < rdi_network_q] <- 0\n  \n  graph_res <- igraph::graph_from_adjacency_matrix(as.matrix(rdi_network_q_process), mode = 'directed', weighted = T)\n  \n  # plot using ggraph\n  dg <- decompose.graph(graph_res)\n  e <- as.data.frame(get.edgelist(dg[[1]])); colnames(e) <- c('Source', 'Target'); e$type <- experiment\n  \n  write.table(file = paste0('/Users/xqiu/Dropbox (Personal)/Projects/Causal_network/causal_network/Cytoscape/', \n                            experiment, '_edge.txt'), e, sep = '\\t', quote = F, col.names = T, row.names = F)\n  \n  return(e)\n}\nDPT <- function (dm, tips = random_root(dm), ..., w_width = 0.1){\n  if (!is(dm, \"DiffusionMap\")) \n    stop(\"dm needs to be of class DiffusionMap, not \", class(dm))\n  if (!length(tips) %in% 1:3) \n    stop(\"you need to spcify 1-3 tips, got \", length(tips))\n  dpt <- destiny:::dummy_dpt(dm)\n  all_cells <- seq_len(nrow(dpt))\n  stats <- destiny:::tipstats(dpt, all_cells, tips)\n  branches <- auto_branch(dpt, all_cells, stats, w_width)\n  colnames(branches$branch) <- paste0(\"Branch\", seq_len(ncol(branches$branch)))\n  colnames(branches$tips) <- paste0(\"Tips\", seq_len(ncol(branches$tips)))\n  dpt@branch <- branches$branch\n  dpt@tips <- branches$tips\n  dpt\n}\nauto_branch <- function (dpt, cells, stats, w_width, nmin = 10L, gmin = 1) {\n  n <- length(cells)\n  stopifnot(n >= nmin)\n  stopifnot(stats$g >= gmin)\n  branches <- destiny:::cut_branches(dpt[cells, stats$tips], cells, w_width)\n  branch <- matrix(destiny:::idx_list_to_vec(branches, cells, n), n,\n                   1L)\n  tips <- matrix(logical(n), n, 1L)\n  tips[match(stats$tips, cells), 1L] <- TRUE\n  subs <- mapply(function(idx_sub, i) {\n    if (length(idx_sub) < nmin || !i %in% idx_sub)\n      return(NULL)\n    sub_stats <- destiny:::tipstats(dpt, idx_sub, i)\n    if (sub_stats$g < gmin)\n      return(NULL)\n    auto_branch(dpt, idx_sub, sub_stats, w_width, nmin, gmin)\n  }, branches, stats$tips, SIMPLIFY = FALSE)\n  nonnull_subs <- vapply(subs, Negate(is.null), logical(1L))\n  if (any(nonnull_subs)) {\n    n_sublevels <- do.call(max, lapply(subs[nonnull_subs],\n                                       function(s) ncol(s$branch)))\n    branch <- cbind(branch, matrix(NA_integer_, n, n_sublevels))\n    tips <- cbind(tips, matrix(NA, n, n_sublevels))\n    for (s in which(nonnull_subs)) {\n      sub <- subs[[s]]\n      idx_sub <- branches[[s]]\n      idx_newcol <- seq.int(ncol(branch) - n_sublevels +\n                              1L, length.out = ncol(sub$branch))\n      stopifnot(ncol(sub$branch) == ncol(sub$tips))\n      branch_offset <- max(branch, na.rm = TRUE)\n      branch[match(idx_sub, cells), idx_newcol] <- sub$branch +\n        branch_offset\n      tips[match(idx_sub, cells), idx_newcol] <- sub$tips\n    }\n  }\n  stopifnot(ncol(branch) == ncol(tips))\n  list(branch = branch, tips = tips)\n}\nrun_new_dpt <- function(cds, norm_method = 'none', root = NULL, color_by = 'Cell_type'){\n  message('root should be the id to the cell not the cell name ....')\n  norm_data <- monocle:::normalize_expr_data(cds, norm_method = norm_method)\n  norm_data <- t(norm_data)\n  duplicated_genes <- which(base::duplicated.array(norm_data))\n  norm_data[duplicated_genes, 1] <- norm_data[duplicated_genes, 1] + rnorm(length(duplicated_genes), 0, 1)\n  dm <- DiffusionMap(norm_data)\n  dpt <- DPT(dm)\n  ts <- dm@transitions\n  M <- destiny:::accumulated_transitions(dm)\n  if(is.null(root)){\n  }\n  else{\n    dm <- DiffusionMap(norm_data)\n    dpt <- DPT(dm, tips = root)\n  }\n  if('Hours' %in% colnames(pData(cds)))\n    pData(cds)$Hours <- pData(cds)[, color_by]\n  p1 <- qplot(DM$DC1, DM$DC2, colour = pData(cds)$Hours)\n  branch <- dpt@branch\n  if(is.null(root))\n    root <- which.min(pData(cds)$Pseudotime)\n  pt <- dpt[root, ]\n  dp_res <- list(dm = dm, pt = pt, ts = ts, M = M, ev = dm@eigenvectors, p1 = p1, branch = branch)\n  return(dp_res)\n}\n\n# function to calculate the AUC / ROC curve for \ncalStatistics <- function(data, reference_network_pvals, sd = 0) {\n  run_ids <- as.character(unique(data$V2)) #get the sample RUN ids\n  \n  # collect the RDI and cRDI network for each run\n  RDI_parallel_res_list <- list()\n  cRDI_parallel_res_list <- list()\n  \n  # run RDI / cRDI for each sampled run\n  print(run_ids)\n  for(id in run_ids) {\n    message(id)\n    \n    subset_exprs_mat <- as.matrix(subset(data, V2 == id)[, -c(1:2)]) # remove first two columns: gene_name / run\n    noise = matrix(rnorm(mean = 0, sd = sd, nrow(subset_exprs_mat) * ncol(subset_exprs_mat)), nrow = nrow(subset_exprs_mat)) # add noise to break tie\n    \n    run_vec <- rep(1:1, each = ncol(subset_exprs_mat))\n    tmp <- expand.grid(1:nrow(subset_exprs_mat), 1:nrow(subset_exprs_mat), stringsAsFactors = F)\n    super_graph <- tmp[tmp[, 1] != tmp[, 2], ] - 1 # convert to C++ index\n    \n    subset_exprs_mat_noise <- subset_exprs_mat + noise\n    # subset_exprs_mat_noise <- subset_exprs_mat_noise - min(subset_exprs_mat_noise)\n    \n    a <- Sys.time() # calculate RDI\n    RDI_parallel_res <- calculate_rdi_cpp_wrap(t(subset_exprs_mat_noise), super_graph = as.matrix(super_graph), delays = c(1), turning_points = 0, method = 1)\n    b <- Sys.time()\n    \n    a <- Sys.time() # calculate cRDI\n    cRDI_parallel_res <- calculate_conditioned_rdi_cpp_wrap(t(subset_exprs_mat_noise), super_graph = as.matrix(super_graph),\n                                                            max_rdi_value = RDI_parallel_res$max_rdi_value, max_rdi_delays = RDI_parallel_res$max_rdi_delays, k = 1)\n    b <- Sys.time()\n    \n    # # make a list of list\n    RDI_parallel_res_list <- c(RDI_parallel_res_list, list(RDI_parallel_res$max_rdi_value))\n    cRDI_parallel_res_list <- c(cRDI_parallel_res_list, list(cRDI_parallel_res))\n  }\n  \n  # extract only the max_rdi_values\n  list_len <- length(RDI_parallel_res_list)\n  RDI_parallel_res_list <- RDI_parallel_res_list\n  RDI_res_df <- process_data(RDI_parallel_res_list); RDI_res_df <- t(do.call(rbind.data.frame, RDI_res_df))\n  cRDI_res_df <- process_data(cRDI_parallel_res_list); cRDI_res_df <- t(do.call(rbind.data.frame, cRDI_res_df))\n  \n  colnames(RDI_res_df) <- paste0(\"cluster_\", 1:ncol(RDI_res_df))\n  colnames(cRDI_res_df) <- paste0(\"cluster_\", 1:ncol(cRDI_res_df))\n  \n  # calculate the ROC / AUC values\n  RDI_df <- calROCAUC(RDI_res_df)\n  cRDI_df <- calROCAUC(cRDI_res_df)\n  \n  # return the results\n  return(list(RDI_df = RDI_df, cRDI_df = cRDI_df))\n}\n\n# calculate the ROC / AUC values RDI network (in columns) using the reference network \ncalROCAUC <- function(res_df, network = neuron_network) {\n  gene_uniq <- unique(c(as.character(neuron_network[, 1]), as.character(neuron_network[, 2])))\n  all_cmbns <- expand.grid(gene_uniq, gene_uniq)\n  valid_all_cmbns <- all_cmbns[all_cmbns$Var1 != all_cmbns$Var2, ]\n  valid_all_cmbns_df <- data.frame(pair = paste(tolower(valid_all_cmbns$Var1), tolower(valid_all_cmbns$Var2), sep = '_'), pval = 0)\n  row.names(valid_all_cmbns_df) <- valid_all_cmbns_df$pair\n  valid_all_cmbns_df[paste(tolower(neuron_network$V1), tolower(neuron_network$V2), sep = '_'), 2] <- 1\n  \n  reference_network_pvals <- valid_all_cmbns_df[, 2]\n  p_thrsld <- 0\n  \n  roc_df_list <- lapply(colnames(res_df), function(x, reference_network_pvals_df = reference_network_pvals) {\n    pvals <- res_df[, x]\n    \n    pvals[is.na(pvals)] <- 0\n    reference_network_pvals[is.na(reference_network_pvals)] <- 0\n    pvals <- (pvals - min(pvals)) / (max(pvals) - min(pvals))\n    res <- generate_roc_df(pvals, reference_network_pvals > p_thrsld)\n    colnames(res) <- c('tpr', 'fpr', 'auc')\n    cbind(res, method = x)\n  })\n  \n  roc_df_list <- lapply(roc_df_list, function(x) {colnames(x) <- c('tpr', 'fpr', 'auc', 'method'); x} )\n  roc_df <- do.call(rbind, roc_df_list)\n  \n  return(roc_df)  \n}\n\n# process the RDI / cRDI result for each sample so that we get a vector of RDI/cRDI values for each sample \nprocess_data <- function(parallel_res_list, network = neuron_network, genes = c('Pax6', 'Mash1', 'Brn2', 'Zic1', 'Tuj1', 'Hes5', 'Scl', 'Olig2', 'Stat3', 'Myt1L', 'Aldh1L', 'Sox8', 'Mature')) {\n  if(nrow(parallel_res_list[[1]]) != length(genes))\n    stop(\"please provide the correct gene vector\")\n  \n  processed_res <- lapply(parallel_res_list, function(x) {\n    gene_uniq <- unique(c(as.character(network$V1), as.character(network$V2)))\n    all_cmbns <- expand.grid(gene_uniq, gene_uniq)\n    valid_all_cmbns <- all_cmbns[all_cmbns$Var1 != all_cmbns$Var2, ]\n    all_valid_gene_pairs <- paste(tolower(valid_all_cmbns$Var1), tolower(valid_all_cmbns$Var2), sep = '_')\n    \n    dimnames(x) <- list(genes, genes)\n    mlt_RDI_benchmark_res <- melt(as.matrix(x)) # [1:12, 1:12]\n    row.names(mlt_RDI_benchmark_res) <- paste(tolower(mlt_RDI_benchmark_res$Var1), tolower(mlt_RDI_benchmark_res$Var2), sep = '_')\n    \n    t(data.frame(RDI = mlt_RDI_benchmark_res[all_valid_gene_pairs, 'value']))\n  })\n  return(processed_res)\n}\n\n\n# \n",
    "created" : 1507157493495.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2899462122",
    "id" : "BD2D13DB",
    "lastKnownWriteTime" : 1507157395,
    "last_content_update" : 1507157395,
    "path" : "~/Dropbox (Personal)/Projects/Causal_network/causal_network/./Scripts/function.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 9,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}